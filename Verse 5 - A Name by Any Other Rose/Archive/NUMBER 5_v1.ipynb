{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d6c019c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.7.4\n",
      "  latest version: 24.3.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=24.3.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!conda install --prefix {sys.prefix} -y -c pytorch pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5b3ecda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\weber\\anaconda3\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\weber\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\weber\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\weber\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\weber\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\weber\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\weber\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\weber\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\weber\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\weber\\anaconda3\\lib\\site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\weber\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\weber\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\weber\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\weber\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\weber\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\weber\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\weber\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\weber\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c327cc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Weber\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cd686cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68e0808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ead3d4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Two roads diverged in a yellow wood, and police told them two other motorists jumped into a ditch in an apparent attempt to cut traffic.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Two roads diverged in a yellow wood, and\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16f15e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Two roads diverged in a yellow wood, and the man who was behind the scene of that accident has been identified as a 37-year-old from Gersfield. Authorities believe a 17-year-old boy was able to run away with'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Two roads diverged in a yellow wood, and\")[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb3539f",
   "metadata": {},
   "source": [
    "And then you can generate text with the pipeline. The first argument is the prompt; any remaining parameters will be forwarded to the model's `.generate()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f7c8e6a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Two roads diverged in a yellow Ford GT and the Audi A3 GT, where a pair of people were reported missing in the busy Chico, South Carolina town of Carrizo County.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Two roads diverged in a yellow\",\n",
    "          max_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f4cc74",
   "metadata": {},
   "source": [
    "As a reminder, to get the actual generated text, use indexing to get the value of the dictionary in the list returned from the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "93ef297e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Two roads diverged in a yellow haze of clouds over the north of the country, causing the death of 14 people and one woman on the road. \"Nobody was hurt,\" a man said. \"At least six people were killed. On Wednesday morning, no one was injured, so a couple were injured in the blast,\" the mayor said. \"It seemed that some people were killed. We had to find the way home. We were very much in shock. It was very hard to do as'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Two roads diverged in a yellow\",\n",
    "          max_length=100)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebc65022",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "my_poem=generator(\"Two roads diverged in a yellow\",\n",
    "          max_length=100)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "00ae8255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Two roads divEEEEEErgEEEEEEd in a yEEEEEEllow BMW S-SEEEEEEriEEEEEEs.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMt-T-D-N-R-T-T-E-N-R\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_poem.replace(\"e\", \"EEEEEE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e84bb46",
   "metadata": {},
   "source": [
    "## Controlling the model\n",
    "\n",
    "By default, the `distilgpt2` model samples from the possible next tokens, weighted by the probability assigned to that token. This strategy leads to text that shows a good deal of variety, but there are strategies that we can use and parameters that we can tweak to exert a little more control over the model's output. In this section, I show a few of these strategies.\n",
    "\n",
    "### The magic of the prompt\n",
    "\n",
    "Transformer models are often able to follow up on cues you give about the desired content and style of the text in the prompt itself. The smaller transformer models aren't especially good at this, but it's still worth playing around with. For example, to get `distilgpt2` to generate something that looks like a movie review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9f51c882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My review of The Road Not Taken, the Movie: a New Movie from Hollywood›..\n",
      "The Road Not Taken, the Movie: a New Movie from Hollywood›\n",
      "The Road Not Taken, the Movie: a New Movie from Hollywood›\n",
      "The Road Not Taken, the Movie: a New Movie from Hollywood›\n",
      "The Road Not Taken, the Movie: a New Movie from Hollywood›\n",
      "If you haven't read the movie so far already, here's the top\n"
     ]
    }
   ],
   "source": [
    "print(generator(\"My review of The Road Not Taken, the Movie:\", max_length=100)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d643be14",
   "metadata": {},
   "source": [
    "You can also generate dialogues and interview transcripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0295ee5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allison: I took the road less traveled by.\n",
      "Robert Frost: I was going about the same thing. My whole life was about getting out the doors. That was what I wanted me to do. I wanted my car to be as safe and as safe as mine. And that's when I did exactly the opposite.\n",
      "Joss Whedon: I had a lot of fun. I was on a plane and I looked at him and said to me that I love his work. So I\n"
     ]
    }
   ],
   "source": [
    "print(generator(\"Allison: I took the road less traveled by.\\nRobert Frost:\", max_length=100)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06de3603",
   "metadata": {},
   "source": [
    "Poetry facts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cfc9a4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My favorite facts about poetry:\n",
      "\n",
      "1. The last time you read the song, you're going to remember that there was a time when no one wanted you to read the poem.\n",
      "2. The number of times you're going to read the verse, you're going to remember that, 'B.S.W.'\"\n",
      "3. The number of times you're going to read the verse, you're going to remember that, 'W.'N.B.,\" said the poet.\n"
     ]
    }
   ],
   "source": [
    "print(generator(\"My favorite facts about poetry:\\n\\n1.\",\n",
    "                max_length=100)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e80b074",
   "metadata": {},
   "source": [
    "In general, this kind of prompting works best with texts that are likely to have a lot of representation in the training corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81363f7a",
   "metadata": {},
   "source": [
    "### Sampling with temperature\n",
    "\n",
    "As mentioned above, the `distilgpt2` model, by default, picks the next token at random, weighted by the probability that the model assigns to the word. To demonstrate how this works, let's imagine that the model only has five tokens in its vocabulary (instead of 50,000+). A schematic illustration of those probabilities might look like this:\n",
    "\n",
    "    prompt: Whose woods these are I think I\n",
    "    probabilities:\n",
    "        know -> 0.5\n",
    "        knew -> 0.2\n",
    "        smell -> 0.15\n",
    "        see -> 0.1\n",
    "        am -> 0.05\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15474aa6",
   "metadata": {},
   "source": [
    "The probabilities will add up to `1.0`. A probability of `0.5` indicates that the token has a 50% probability of coming next; a probability of 0.2 means that the token has a 20% probability of coming next, etc. Here are those probabilities represented in Python as two lists—one with the words, and one with the probabilities that correspond to those words by index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "67854938",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['know', 'knew', 'smell', 'see', 'am']\n",
    "probs = [0.5, 0.2, 0.15, 0.1, 0.05]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95c6404",
   "metadata": {},
   "source": [
    "By default, to select the next token, the generation code picks from this list weighted by probability. The code to do this with PyTorch looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b9f85011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "know\n"
     ]
    }
   ],
   "source": [
    "index = torch.multinomial(torch.tensor(probs), 1).item()\n",
    "print(tokens[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636e2d8c",
   "metadata": {},
   "source": [
    "You don't have to worry about the specifics of this code—I'm just using it to demonstrate how the sampling process works. Run the code a few times and you'll see that about half the time you get \"knew\"—the token with the highest probability. Running the code in a loop makes this a bit easier to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "af732dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "know\n",
      "smell\n",
      "knew\n",
      "smell\n",
      "see\n",
      "know\n",
      "smell\n",
      "knew\n",
      "know\n",
      "know\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    index = torch.multinomial(torch.tensor(probs), 1).item()\n",
    "    print(tokens[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12d2fcf",
   "metadata": {},
   "source": [
    "The generation process has a parameter called *temperature*, which lets you shift the probability distribution of the next token before it's sampled. If the temperature parameter is `1.0`, then sampling will proceed as normal, with the tokens weighted by their estimated probability. If the temperature parameter is less than `1.0`, then tokens that were already probable will get *more* probable. If the temperature parameter is greater than `1.0`, then the probabilities start to even out, approaching a uniform distribution (meaning that no token is more likely to be chosen than any other). To demonstrate this, I've written some code below that applies temperature to the probabilities defined above, and shows the resulting changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2fa5775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature 0.10\n",
      "know   → 1.00\n",
      "knew   → 0.00\n",
      "smell  → 0.00\n",
      "see    → 0.00\n",
      "am     → 0.00\n",
      "\n",
      "temperature 0.35\n",
      "know   → 0.90\n",
      "knew   → 0.07\n",
      "smell  → 0.03\n",
      "see    → 0.01\n",
      "am     → 0.00\n",
      "\n",
      "temperature 1.00\n",
      "know   → 0.50\n",
      "knew   → 0.20\n",
      "smell  → 0.15\n",
      "see    → 0.10\n",
      "am     → 0.05\n",
      "\n",
      "temperature 2.00\n",
      "know   → 0.34\n",
      "knew   → 0.21\n",
      "smell  → 0.19\n",
      "see    → 0.15\n",
      "am     → 0.11\n",
      "\n",
      "temperature 50.00\n",
      "know   → 0.20\n",
      "knew   → 0.20\n",
      "smell  → 0.20\n",
      "see    → 0.20\n",
      "am     → 0.20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for temperature in [0.1, 0.35, 1.0, 2.0, 50.0]:\n",
    "    modified = torch.softmax(\n",
    "        torch.log(torch.tensor(probs)) / temperature, dim=-1)\n",
    "    print(f\"temperature {temperature:0.02f}\")\n",
    "    for tok, prob in zip(tokens, modified):\n",
    "        print(tok.ljust(6), \"→\", f\"{prob:0.002f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c095c57",
   "metadata": {},
   "source": [
    "You can see that at temperature `1.0`, the probabilities are identical to the original. At temperature `0.35`, the probability of the most likely token has been boosted, but the other tokens still have a small chance of occurring. At temperature `0.1`, only the most likely token has a chance of being selected. At temperature `2.0`, the most likely token is still the most likely, but the probabilities of the other tokens have been boosted in comparison; at temperature `50.0`, no token is considered to be more likely than any other.\n",
    "\n",
    "To apply temperature sampling to the model when generating text, pass the `temperature` parameter to the pipeline, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "13c5bccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Two roads diverged in a yellow light, and the police were called to the scene.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Two roads diverged in a yellow\",\n",
    "          temperature=0.1,\n",
    "          max_length=100)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b94bd84",
   "metadata": {},
   "source": [
    "Low temperatures generally produce predictable, repetitive results. Here's an attempt with high temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "93298377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Two roads diverged in a yellow BMW i1.'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Two roads diverged in a yellow\",\n",
    "          temperature=4.0,\n",
    "          max_length=100)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a379f7",
   "metadata": {},
   "source": [
    "The higher temperature example produces less likely sequences of words, so the text is a bit livelier—sometimes at the cost of coherence.\n",
    "\n",
    "Adjusting the temperature can be useful when you want the text to be more or less \"weird.\" It can be helpful to adjust the temperature downward when you feel as though the model is producing text that is a bit too unpredictable; it can be helpful to adjust upward when you want to model to take more unexpected turns when generating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b2500c",
   "metadata": {},
   "source": [
    "### Top-k sampling\n",
    "\n",
    "By default, the generation process only selects from the top 50 most probable tokens at each step. This is called \"top-k filtering.\" Because of top-k filtering, you're not likely to sample truly unusual tokens even when the temperature is high. You can adjust the threshold for top-k filtering with the `top_k` parameter of the model. For example, adjusting `top_k` to the number of items in the vocabulary ensures that every token gets its chance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bf4d73cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Two roads diverged in a yellow plume, coming from a maintenance spot on his chain, The Express.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTri Lowe Police say he brought a cellphone in for a safety check to check who was making a threat to the property after he woke up on the street on Wednesday afternoon due to a fire.\\n\\nThe incident focused on the offence by hitting a Mazda 2016sized car.\\n\\nUPI'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Two roads diverged in a yellow\",\n",
    "          top_k=tokenizer.vocab_size,\n",
    "          max_length=100)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa7b265",
   "metadata": {},
   "source": [
    "Using this with a temperature greater than `1.0` can yield some unusual turns of phrase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0b690453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Two roads diverged in a yellow Mazda V-7 however driver Kim Whenerman vs. G differences might make the difference since Red assumes CPS can eat a heavier proportion of fuel compared with road racing cars – although Driving Do Cute uses class C sales confer less loads, this used 2 litres toatisfactor that poll to shopping district total, probably less nor much latter traffic density sow confusion and and vacancies on Tuesday As is often the case with CPS significant interventions, the GM-CCS met with friends'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Two roads diverged in a yellow\",\n",
    "          top_k=tokenizer.vocab_size,\n",
    "          temperature=1.2,\n",
    "          max_length=100)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1843eaef",
   "metadata": {},
   "source": [
    "On the other extreme, setting the `top_k` value to `1` ensures that *only* the most likely token is chosen at each step. This is the same thing as [\"greedy decoding\"](https://huggingface.co/transformers/main_classes/model.html#transformers.generation_utils.GenerationMixin.greedy_search):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ef3b18b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Two roads diverged in a yellow light, and the police were called to the scene.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Two roads diverged in a yellow\",\n",
    "          top_k=1,\n",
    "          max_length=100)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e343fed",
   "metadata": {},
   "source": [
    "Playing around with `top_k` and `temperature` in tandem is a good way to make adjustments to the texture of your generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d87511e",
   "metadata": {},
   "source": [
    "### Logit warping: Exclude \"bad\" words\n",
    "\n",
    "The `.generate()` method has a parameter called `bad_words_ids`, which causes the model to zero out the probabilities of tokens associated with words that you pass in. The intended use of this feature is to stop the model from generating offensive or harmful words. But we can also repurpose it for poetic purposes. For example, in the cell below, I make the model complete the prompt \"It was a dark and stormy\" *without* using the words \"night\" or \"day\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e691ce3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"It was a dark and stormy storm that set off the town of Nizke. The village of Nizke was deserted and abandoned. The village's people had fled, but they did not understand it. Then when they could finally reach N\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"It was a dark and stormy\",\n",
    "          bad_words_ids=tokenizer([\" night\", \" day\"]).input_ids)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdc646c",
   "metadata": {},
   "source": [
    "The syntax for specifying the \"bad words\" is to call the tokenizer on a list of words that you want to exclude, and then get the `.input_ids` attribute of the value returned from calling the tokenizer. This yields a list of lists that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8a18221f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3237, 1653], [47, 3258, 680]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"Allison\", \"Parrish\"]).input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44e2879",
   "metadata": {},
   "source": [
    "Note that I used ` night` and ` day` as the words, with leading spaces—this is necessary because I ended the prompt without whitespace, so the model is likely to generate a token with leading whitespace at the next step. I've found that the `bad_words_ids` parameter works best if your list of words includes versions both with and without whitespace.\n",
    "\n",
    "Here's another example: getting the model to complete a prompt without using any forms of the verb *to be*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a5c7fc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a time, on the other end of the planet at the top of a galaxy, it became apparent that the star, the Milky Way, had the same gravitational background as the stars in this study. When those planets moved at great distances from the outside, they made much larger changes to their properties. However, once they reached the core of the Milky Way they began to move farther and farther in the opposite direction, and that distance gained increased further. Finally, before reaching the core of the'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Once upon a time,\",\n",
    "          bad_words_ids=tokenizer(\n",
    "              [\"be\", \" be\",\n",
    "               \"am\", \" am\",\n",
    "               \"are\", \" are\",\n",
    "               \"is\", \" is\",\n",
    "               \"was\", \" was\",\n",
    "               \"were\", \" were\"]).input_ids,\n",
    "          max_length=100)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec6a2df",
   "metadata": {},
   "source": [
    "You can also create a list of token IDs that you want to exclude on the fly. In the following example, I make a list of token IDs that have the letter `e` in them, and pass that list to the `bad_words_ids` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9dd76bc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last month, I saw a lot of this, and I think it shows just how hard it is to avoid using Bitcoin just by looking at how far off that block diagram can go for Bitcoin's full functionality. First off, I had to go through a lot of data mining, which is why I want to start to look at a small part of why Bitcoin is a cash-only, anonymous, and anonymous Bitcoin.\n",
      "\n",
      "For my first half hour and hour of work, I had to wait\n"
     ]
    }
   ],
   "source": [
    "forbidden_ids = []\n",
    "for key, val in tokenizer.get_vocab().items():\n",
    "    if 'e' in key:\n",
    "        forbidden_ids.append([val]) # needs to be a list of lists\n",
    "print(generator(\"Last month, I\",\n",
    "          bad_words_ids=forbidden_ids,\n",
    "          max_length=100)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db6369e",
   "metadata": {},
   "source": [
    "### Fine-tuning a model\n",
    "\n",
    "\"Fine-tuning\" is a way of slightly modifying a model by training it a few extra steps on a corpus of your choice. This process adjusts the probabilities of the model so that it more closely reflects the probabilities of the source text you train it on. Fine-tuning models with Transformers is a little bit tricky! First, you'll need to install Hugging Face's `datasets` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e3757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e0395a",
   "metadata": {},
   "source": [
    "And then import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6f4d68e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab81720",
   "metadata": {},
   "source": [
    "You'll want to select a text file to fine-tune the model on. Fine-tuning works best on large amounts of text, but fine-tuning is also very slow if you're not using a GPU. For demonstration purposes, I create a special version of [Frankenstein](https://www.gutenberg.org/ebooks/84) that contains only the first 20000 characters, and save it to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "35a2a3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"84-0-20k.txt\", \"w\") as fh:\n",
    "    fh.write(open(\"84-0.txt\").read()[:20000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1221ac91",
   "metadata": {},
   "source": [
    "Then I load this text file as my fine-tuning dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9a037218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-8396e67af902a1a6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /Users/allison/.cache/huggingface/datasets/text/default-8396e67af902a1a6/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /Users/allison/.cache/huggingface/datasets/text/default-8396e67af902a1a6/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.load_dataset('text', data_files=\"84-0-20k.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31f0ead",
   "metadata": {},
   "source": [
    "Now, there's a bunch of obligatory processing that we need to do to the data in order to prepare it for the model. This is boilerplate stuff, which I'm not going to go into in detail. If you want details, consult Hugging Face's [fine-tuning language models notebook](https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb).\n",
    "\n",
    "First, we tokenize the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc75a5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3725218132264e899ab325843a97800a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=484.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_training_data = training_data.map(\n",
    "    lambda x: tokenizer(x['text']),\n",
    "    remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea41f005",
   "metadata": {},
   "source": [
    "Then we break the tokenized text up into batches of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "595b9bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e480a0a0dd946e6ad0b0f9099abb724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "block_size = 64\n",
    "# magic from https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "lm_training_data = tokenized_training_data.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e522074",
   "metadata": {},
   "source": [
    "Now we import the `Trainer` class, which implements a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eee70cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12058c0",
   "metadata": {},
   "source": [
    "Running the following cell creates the `Trainer` object. The `output_dir` parameter specifies a directory where your fine-tuned model will be saved. The `num_train_epochs` sets how many \"epochs\" the trainer will run; one epoch is one iteration over the entire dataset. More epochs is better, but even one epoch can significantly change the way the model generates text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6d5359f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  train_dataset=lm_training_data['train'],\n",
    "                  args=TrainingArguments(\n",
    "                      output_dir='distilgpt2-finetune-frankenstein20k',\n",
    "                      num_train_epochs=1,\n",
    "                      do_train=True,\n",
    "                      do_eval=False\n",
    "                  ),\n",
    "                  tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22076634",
   "metadata": {},
   "source": [
    "Finally, the cell below will start the training process. If you're running this on a computer without a GPU, it will take a while. You can open this notebook on [Google Colab](http://colab.research.google.com/) if you want and take advantage of the free GPU that Google lets you use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a0a974bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 67\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:50, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9, training_loss=4.9099070231119795, metrics={'train_runtime': 56.9456, 'train_samples_per_second': 1.177, 'train_steps_per_second': 0.158, 'total_flos': 2107446755328.0, 'train_loss': 4.9099070231119795, 'epoch': 1.0})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda411f7",
   "metadata": {},
   "source": [
    "Running the cell below will save the model to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f18e00fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to distilgpt2-finetune-frankenstein20k\n",
      "Configuration saved in distilgpt2-finetune-frankenstein20k/config.json\n",
      "Model weights saved in distilgpt2-finetune-frankenstein20k/pytorch_model.bin\n",
      "tokenizer config file saved in distilgpt2-finetune-frankenstein20k/tokenizer_config.json\n",
      "Special tokens file saved in distilgpt2-finetune-frankenstein20k/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474a59af",
   "metadata": {},
   "source": [
    "Now you can generate with the fine-tuned model! The fine-tuning process modifies the model in-place, so the `pipeline` you created before will make use of the fine-tuned model. (Note that if you want to get the original `distilgpt2` back, you'll need to reload it with the `.from_pretrained()` method, as demonstrated at the top of the notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6f7769a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Two roads diverged in a yellow.\\n\\n\\n\\nAt a glance, I could not say what I was. I tried to hold my breaths, and felt a sense of peace that seemed to me an odd thing. I tried to keep my composure.\\nThe distance between them was far beyond my comprehension, and so, then, seemed to the most of my life. As if I had always wished to stay home, as though the weather was different, I had no space for my own'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Two roads diverged in a yellow\", max_length=100)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798333f5",
   "metadata": {},
   "source": [
    "You can see that fine-tuning on even a small dataset produces big changes in the model.\n",
    "\n",
    "If you want to use your fine-tuned model in another project, use the same syntax that we used above to load `distilgpt2`—just replace `distilgpt2` with the name of the directory where you saved your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ed9960c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file distilgpt2-finetune-frankenstein20k/added_tokens.json. We won't load it.\n",
      "loading file distilgpt2-finetune-frankenstein20k/vocab.json\n",
      "loading file distilgpt2-finetune-frankenstein20k/merges.txt\n",
      "loading file distilgpt2-finetune-frankenstein20k/tokenizer.json\n",
      "loading file None\n",
      "loading file distilgpt2-finetune-frankenstein20k/special_tokens_map.json\n",
      "loading file distilgpt2-finetune-frankenstein20k/tokenizer_config.json\n",
      "loading configuration file distilgpt2-finetune-frankenstein20k/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"distilgpt2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": true,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_length\": 50,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file distilgpt2-finetune-frankenstein20k/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at distilgpt2-finetune-frankenstein20k.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "my_tokenizer = AutoTokenizer.from_pretrained('distilgpt2-finetune-frankenstein20k')\n",
    "my_model = AutoModelForCausalLM.from_pretrained('distilgpt2-finetune-frankenstein20k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c62d434",
   "metadata": {},
   "source": [
    "Now generate with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "201ab0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_generator = pipeline(\"text-generation\", model=my_model, tokenizer=my_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8de43360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Two roads diverged in a yellowish haze, and there were only two clear roadways crossing east of the river. As the sky fell and the ground was darkened, the wind blew a terrible wind; but it soon returned to the east where he'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_generator(\"Two roads diverged in a yellow\")[0]['generated_text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
